{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57226901",
   "metadata": {},
   "source": [
    "# Actuarial Standards of Practice (ASOP) Q&A Machine using Retrieval Augmented Generation (RAG)\n",
    "This project aims to create a Retrieval-Augmented Generation (RAG) process for actuaries to ask questions on a set of Actuarial Standards of Practice (ASOP) documents. The RAG process utilizes the power of the Large Language Model (LLM) to provide answers to questions on ASOPs.\n",
    "\n",
    "However, RAG is not without challenges, i.e., hallucination and inaccuracy. This code allows verifiability by providing the context it used to arrive at those answers. This process enables actuaries to validate the information provided by the LLM, empowering them to make informed decisions. By combining the capabilities of LLM with verifiability, this code offers actuaries a robust tool to leverage LLM technology effectively and extract maximum value.\n",
    "\n",
    "The current example uses either OpenAI's GPT 3.5 turbo or a local LLM. Using local LLM can address potential data privacy or security concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c8d88",
   "metadata": {},
   "source": [
    "# 1. Initial Setup\n",
    "This setup includes loading environment variables from a `.env` file, setting the required environment variables, and importing the necessary modules for further processing. It ensures that the code has access to the required APIs and functions for the subsequent tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06fc759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial set up\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the variables from .env file and set the API key (or user may manually set the API key)\n",
    "load_dotenv()  # This loads the variables from .env (not part of repo)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" # use when you want to debug or monitor the performance of your langchain applications\n",
    "#os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGCHAIN_API_KEY') # use when accessing cloud-based language models or services that langchain integrates with\n",
    "\n",
    "# Import the necessary modules\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel # for RAG with source\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import glob\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35630ca8-a707-4445-b8a2-661fe3312d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 45.9M/45.9M [00:00<00:00, 71.2MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "# Change to False if using local models instead of OpenAI models\n",
    "use_OpenAI = False\n",
    "\n",
    "if use_OpenAI:\n",
    "    embeddings_model = OpenAIEmbeddings()\n",
    "    db_directory = \"../data/chroma_db1\"\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", \n",
    "                     temperature=0) # context window size 16k for GPT 3.5 Turbo\n",
    "\n",
    "else: #Open source models used here are for illustration and educational purposes\n",
    "    embeddings_model = GPT4AllEmbeddings()\n",
    "    db_directory = \"../data/chroma_db2\"\n",
    "    # Create a local large language model for augmented generation\n",
    "    # Ollama is one way to easily run inference (especially on macOS)\n",
    "    llm = Ollama(model=\"solar:10.7b-instruct-v1-q5_K_M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04673e4",
   "metadata": {},
   "source": [
    "# 2. Load PDF Files and Convert to a Vector DB\n",
    "1. Create a function to load and extract text from PDF files in a specified folder. It defines a function called `load_pdfs_from_folder()` that takes a folder path as input and returns a list of extracted text documents from the PDF files in that folder.\n",
    "\n",
    "2. In the example, the folder path `../data/ASOP` is used, but you can modify it to point to your desired folder.\n",
    "\n",
    "3. By calling the `load_pdfs_from_folder()` function with the folder path, the code loads the PDF files, extracts the text using the PyPDFLoader, and stores the extracted text documents in the `docs` list.\n",
    "\n",
    "4. After loading and extracting the text, a `RecursiveCharacterTextSplitter` object is created with specific parameters for chunking the documents. The `split_documents()` method is then used to split the documents into smaller chunks based on the specified parameters.\n",
    "\n",
    "5. Finally, a Chroma vectorstore is created from the document splits. The vectorstore uses the defined embedding model for embedding the chunks and is saved to the predefined directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c3b91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only when the DB directory is empty\n",
    "if not os.path.exists(db_directory) or not os.listdir(db_directory):\n",
    "    # Define a function to load and extract text from PDFs in a folder\n",
    "    def load_pdfs_from_folder(folder_path):\n",
    "        # Get a list of PDF files in the specified folder\n",
    "        pdf_files = glob.glob(f\"{folder_path}/*.pdf\")\n",
    "        docs = []\n",
    "        for pdf_file in pdf_files:\n",
    "            # Load the PDF file using the PyPDFLoader\n",
    "            loader = PyPDFLoader(pdf_file) \n",
    "            # Extract the text from the PDF and add it to the docs list\n",
    "            docs.extend(loader.load())\n",
    "        return docs\n",
    "    \n",
    "    # Example folder path\n",
    "    folder_path = '../data/ASOP'\n",
    "    \n",
    "    # Call the function to load and extract text from PDFs in the specified folder\n",
    "    docs = load_pdfs_from_folder(folder_path)\n",
    "    \n",
    "    # Create a text splitter object with specified parameters\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=200,\n",
    "        length_function=len,)\n",
    "    \n",
    "    # Split the documents into chunks using the text splitter\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # Create a Chroma vector database from the document splits, using OpenAIEmbeddings for embedding\n",
    "    vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                        embedding=embeddings_model, \n",
    "                                        persist_directory=db_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46105ba",
   "metadata": {},
   "source": [
    "# 3. Retrieve from the Vector DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9abd0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Chroma vector database with specified parameters\n",
    "vectorstore = Chroma(embedding_function=embeddings_model, \n",
    "                     persist_directory=db_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9744b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve and RAG chain\n",
    "\n",
    "# Create a retriever using the vector database as the search source\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", \n",
    "                                     search_kwargs={'k': 6, 'lambda_mult': 0.25}) \n",
    "# Use MMR (Maximum Marginal Relevance) to find a set of documents that are both similar to the input query and diverse among themselves\n",
    "# Increase the number of documents to get, and increase diversity (lambda mult 0.5 being default, 0 being the most diverse, 1 being the least)\n",
    "\n",
    "# Load the RAG (Retrieval-Augmented Generation) prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define a function to format the documents with their sources and pages\n",
    "def format_docs_with_sources(docs):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    sources_pages = \"\\n\".join(f\"{doc.metadata['source']} (Page {doc.metadata['page'] + 1})\" for doc in docs)\n",
    "    # Added 1 to the page number assuming 'page' starts at 0 and we want to present it in a user-friendly way\n",
    "\n",
    "    return f\"Documents:\\n{formatted_docs}\\n\\nSources and Pages:\\n{sources_pages}\"\n",
    "\n",
    "# Create a RAG chain using the formatted documents as the context\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs_with_sources(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a parallel chain for retrieving and generating answers\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9243e34a",
   "metadata": {},
   "source": [
    "# 4. Generate Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ceedb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output():\n",
    "    # Prompt the user for a question on ASOP\n",
    "    usr_input = input(\"What is your question on ASOP?: \")\n",
    "\n",
    "    # Invoke the RAG chain with the user input as the question\n",
    "    output = rag_chain_with_source.invoke(usr_input)\n",
    "\n",
    "    # Generate the Markdown output with the question, answer, and context\n",
    "    markdown_output = \"### Question\\n{}\\n\\n### Answer\\n{}\\n\\n### Context\\n\".format(output['question'], output['answer'])\n",
    "\n",
    "    last_page_content = None  # Variable to store the last page content\n",
    "    i = 1 # Source indicator\n",
    "\n",
    "    # Iterate over the context documents to format and include them in the output\n",
    "    for doc in output['context']:\n",
    "        current_page_content = doc.page_content.replace('\\n', '  \\n')  # Get the current page content\n",
    "        \n",
    "        # Check if the current content is different from the last one\n",
    "        if current_page_content != last_page_content:\n",
    "            markdown_output += \"- **Source {}**: {}, page {}:\\n\\n{}\\n\".format(i, doc.metadata['source'], doc.metadata['page'], current_page_content)\n",
    "            i = i + 1\n",
    "        last_page_content = current_page_content  # Update the last page content\n",
    "    \n",
    "    # Display the Markdown output\n",
    "    display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54daa6",
   "metadata": {},
   "source": [
    "### Example questions related to ASOPs\n",
    "- explain ASOP No. 14\n",
    "- How are expenses relfected in cash flow testing based on ASOP No. 22?\n",
    "- What is catastrophe risk?\n",
    "- When do I update assumptions?\n",
    "- What should I do when I do not have credible data to develop non-economic assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36183436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is your question on ASOP?:  How are expenses relfected in cash flow testing based on ASOP No. 22?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Question\n",
       "How are expenses relfected in cash flow testing based on ASOP No. 22?\n",
       "\n",
       "### Answer\n",
       "Expenses are reflected in cash flow testing based on ASOP No. 22 through the consideration of administrative or investment fees and other payments borne by the plan, as mentioned in ASOP No. 4's definition of \"expenses.\" The actuary should also adjust the investment return assumption to account for expenses that might not be otherwise recognized when performing cash flow testing on health benefit plans or pension obligations.\n",
       "\n",
       "### Context\n",
       "- **Source 1**: ../data/ASOP/asop022_167.pdf, page 17:\n",
       "\n",
       "July 1991). In addition, in July 1990, the ASB adopted ASOP No. 14, When to Do Cash Flow   \n",
       "Testing for Life and Health Insurance Companies,  to provide guidance in determining whether   \n",
       "or not to do cash flow testing in form ing a professional opinion or recommendation.\n",
       "- **Source 2**: ../data/ASOP/asop007_128.pdf, page 22:\n",
       "\n",
       "draft of ASOP No. 22. A commentator on ASOP No. 22 thought that the definition should include surplus   \n",
       "notes.   The task force agreed and added a definition of “o ther liability cash flows,” which includes a reference   \n",
       "to surplus notes, to both ASOP No. 7 and No. 22.    \n",
       "Section 2.15, Policy Cash Flows (previously section 2.14)   \n",
       "Comment   Response One commentator noted that the definition did not treat  premium taxes properly, as premium taxes are not   \n",
       "paid on behalf of policyholders, but rather are paid as required by law.   The task force agreed with this comment and changed the definition accordingly.    \n",
       "SECTION 3.  ANALYSIS OF ISSUES AND RECOMMENDED PRACTICES   \n",
       "Section 3.2.1, Reasons for Cash Flow Testing, and 3.2.2, Cash Flow Testing is Not Always Necessary   \n",
       "Comment   \n",
       "  Response A few commentators questioned the use of the phrases  “long duration” and “short-term,” and noted that\n",
       "- **Source 3**: ../data/ASOP/asop004_205.pdf, page 11:\n",
       "\n",
       "ASOP No. 4—Doc. No. 205   \n",
       " 4   \n",
       "2.9 Cost Allocation Procedure A procedure that determines the periodic cost  for a plan (for   \n",
       "example, the procedure to determi ne the net periodic pension co st under accounting   \n",
       "standards). The procedure uses an actuarial cost method , and may use an asset valuation   \n",
       "method or an amortization method .    \n",
       " 2.10 Expenses—Administrative or investment fees or other paymen ts borne or expected to be   \n",
       "borne by the plan.    \n",
       " 2.11  \n",
       " Funded Status—Any comparison of a particular measure of plan as sets to a particular   \n",
       "measure of pension obligations.   \n",
       " 2.12 Funding Valuation—A measuremen t of pension obligations or projection of cash flows   \n",
       "performed by the actuary intended to be used by the principal t o determine plan   \n",
       "contributions or to evaluate the adequacy of specified contribu tion levels to support benefit   \n",
       "provisions.    \n",
       " 2.13 Gain and Loss Analysis  \n",
       "—An analysis of the effect on the plan’s funded status  between\n",
       "- **Source 4**: ../data/ASOP/asop022_167.pdf, page 9:\n",
       "\n",
       "ASOP No. 22—September 2001    \n",
       "   \n",
       " 32.8 Cash Flow Testing ⎯A form of cash flow analysis i nvolving the projection and comparison   \n",
       "of the timing and amount of cash flows resulting from economic and other assumptions.    \n",
       "   \n",
       "2.9 Gross Premium Reserve ⎯The actuarial present value of benefits, expenses, and related   \n",
       "amounts less the actuarial present value of premiums and related amounts.   \n",
       " 2.10 Gross Premium Reserve Test  \n",
       "—The comparison of the gross premium reserve computed   \n",
       "under one or more scenarios to the financial statement reserve.   \n",
       " 2.11 Health Benefit Plan  \n",
       "—A contract or other financia l arrangement providing medical,   \n",
       "prescription drug, dental, vision, disability in come, accidental death and dismemberment,   \n",
       "long-term care, or other health-related benefits, whether on a reimbursement, indemnity, or service benefit basis, regardless of the form  of the risk-assuming entity, including health   \n",
       "benefit plans provided by self-insured or government plan sponsors.    \n",
       " 2.12 Insurer\n",
       "- **Source 5**: ../data/ASOP/asop022_167.pdf, page 20:\n",
       "\n",
       "possible.   The task force agreed that the intention was not to exclude other methods from being considered and   \n",
       "clarified the language.    \n",
       "Comment    Response One commentator noted that the wording in the first paragraph did not properly discuss the two separate   \n",
       "issues of testing existing (in force) cash flows vs. testing combined cash flows. On the latter, it was   \n",
       "noted that in some situations changes in asset and liability cash flows may offset.   The task force agreed and expanded upon the expos ure draft wording to make this clearer.\n",
       "- **Source 6**: ../data/ASOP/asop027_197.pdf, page 14:\n",
       "\n",
       "expenses may be paid from plan assets. To the extent such expenses are not   \n",
       "otherwise recognized, the actuary should reduce the investment return   \n",
       "assumption to reflect these expenses.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5938771",
   "metadata": {},
   "source": [
    "# 5. References\n",
    "- https://www.actuarialstandardsboard.org/standards-of-practice/\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/quickstart\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/sources\n",
    "- https://chat.langchain.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd6605-deab-4be4-afaf-563098577bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
