{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57226901",
   "metadata": {},
   "source": [
    "# Actuarial Standards of Practice (ASOP) Q&A Machine using Retrieval Augmented Generation (RAG)\n",
    "This project aims to create a Retrieval-Augmented Generation (RAG) process for actuaries to ask questions on a set of Actuarial Standards of Practice (ASOP) documents. The RAG process utilizes the power of the Large Language Model (LLM) to provide answers to questions on ASOPs.\n",
    "\n",
    "However, RAG is not without challenges, i.e., hallucination and inaccuracy. This code allows verifiability by providing the context it used to arrive at those answers. This process enables actuaries to validate the information provided by the LLM, empowering them to make informed decisions. By combining the capabilities of LLM with verifiability, this code offers actuaries a robust tool to leverage LLM technology effectively and extract maximum value.\n",
    "\n",
    "The current example uses either OpenAI's GPT 3.5 turbo or a local LLM. Using local LLM can address potential data privacy or security concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c8d88",
   "metadata": {},
   "source": [
    "# 1. Initial Setup\n",
    "This setup includes loading environment variables from a `.env` file, setting the required environment variables, and importing the necessary modules for further processing. It ensures that the code has access to the required APIs and functions for the subsequent tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06fc759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial set up\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the variables from .env file and set the API key (or user may manually set the API key)\n",
    "load_dotenv()  # This loads the variables from .env (not part of repo)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" # use when you want to debug or monitor the performance of your langchain applications\n",
    "#os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGCHAIN_API_KEY') # use when accessing cloud-based language models or services that langchain integrates with\n",
    "\n",
    "# Import the necessary modules\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel # for RAG with source\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import glob\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04673e4",
   "metadata": {},
   "source": [
    "# 2. Load PDF Files and Convert to a Vector DB\n",
    "1. Create a function to load and extract text from PDF files in a specified folder. It defines a function called `load_pdfs_from_folder()` that takes a folder path as input and returns a list of extracted text documents from the PDF files in that folder.\n",
    "\n",
    "2. In the example, the folder path `../data/ASOP` is used, but you can modify it to point to your desired folder.\n",
    "\n",
    "3. By calling the `load_pdfs_from_folder()` function with the folder path, the code loads the PDF files, extracts the text using the PyPDFLoader, and stores the extracted text documents in the `docs` list.\n",
    "\n",
    "4. After loading and extracting the text, a `RecursiveCharacterTextSplitter` object is created with specific parameters for chunking the documents. The `split_documents()` method is then used to split the documents into smaller chunks based on the specified parameters.\n",
    "\n",
    "5. Finally, a Chroma vectorstore is created from the document splits. The vectorstore uses the `OpenAIEmbeddings` for embedding the chunks and is persisted to the directory `../data/chroma_db1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3b91d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Uncomment when creating your own vector database for the first time\\n# Define a function to load and extract text from PDFs in a folder\\ndef load_pdfs_from_folder(folder_path):\\n    # Get a list of PDF files in the specified folder\\n    pdf_files = glob.glob(f\"{folder_path}/*.pdf\")\\n    docs = []\\n    for pdf_file in pdf_files:\\n        # Load the PDF file using the PyPDFLoader\\n        loader = PyPDFLoader(pdf_file) \\n        # Extract the text from the PDF and add it to the docs list\\n        docs.extend(loader.load())\\n    return docs\\n\\n# Example folder path\\nfolder_path = \\'../data/ASOP\\'\\n\\n# Call the function to load and extract text from PDFs in the specified folder\\ndocs = load_pdfs_from_folder(folder_path)\\n\\n# Create a text splitter object with specified parameters\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000, \\n    chunk_overlap=200,\\n    length_function=len,)\\n\\n# Split the documents into chunks using the text splitter\\nsplits = text_splitter.split_documents(docs)\\n\\n# Create a Chroma vector database from the document splits, using OpenAIEmbeddings for embedding\\nvectorstore = Chroma.from_documents(documents=splits, \\n                                    embedding=OpenAIEmbeddings(), \\n                                    persist_directory=\"../data/chroma_db1\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Uncomment when creating your own vector database for the first time\n",
    "# Define a function to load and extract text from PDFs in a folder\n",
    "def load_pdfs_from_folder(folder_path):\n",
    "    # Get a list of PDF files in the specified folder\n",
    "    pdf_files = glob.glob(f\"{folder_path}/*.pdf\")\n",
    "    docs = []\n",
    "    for pdf_file in pdf_files:\n",
    "        # Load the PDF file using the PyPDFLoader\n",
    "        loader = PyPDFLoader(pdf_file) \n",
    "        # Extract the text from the PDF and add it to the docs list\n",
    "        docs.extend(loader.load())\n",
    "    return docs\n",
    "\n",
    "# Example folder path\n",
    "folder_path = '../data/ASOP'\n",
    "\n",
    "# Call the function to load and extract text from PDFs in the specified folder\n",
    "docs = load_pdfs_from_folder(folder_path)\n",
    "\n",
    "# Create a text splitter object with specified parameters\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200,\n",
    "    length_function=len,)\n",
    "\n",
    "# Split the documents into chunks using the text splitter\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a Chroma vector database from the document splits, using OpenAIEmbeddings for embedding\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings(), \n",
    "                                    persist_directory=\"../data/chroma_db1\")\n",
    "''' # Uncomment when creating your own vector database for the first time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46105ba",
   "metadata": {},
   "source": [
    "# 3. Retrieve from the Vector DB\n",
    "Once a vector database is created, Section 2 can be commented out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9abd0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Chroma vector database with specified parameters\n",
    "vectorstore = Chroma(embedding_function=OpenAIEmbeddings(), \n",
    "                     persist_directory=\"../data/chroma_db1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9744b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve and RAG chain\n",
    "\n",
    "# Create a retriever using the vector database as the search source\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", \n",
    "                                     search_kwargs={'k': 6, 'lambda_mult': 0.25}) \n",
    "# Use MMR (Maximum Marginal Relevance) to find a set of documents that are both similar to the input query and diverse among themselves\n",
    "# Increase the number of documents to get, and increase diversity (lambda mult 0.5 being default, 0 being the most diverse, 1 being the least)\n",
    "\n",
    "# Load the RAG (Retrieval-Augmented Generation) prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Create a ChatOpenAI language model for augmented generation\n",
    "# llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", \n",
    "#                 temperature=0) # context window size 16k for GPT 3.5 Turbo\n",
    "\n",
    "# Create a local large language model for augmented generation\n",
    "# Ollama is one way to easily run inference (especially on macOS)\n",
    "llm = Ollama(model=\"solar:10.7b-instruct-v1-q5_K_M\")\n",
    "\n",
    "# Define a function to format the documents with their sources and pages\n",
    "def format_docs_with_sources(docs):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    sources_pages = \"\\n\".join(f\"{doc.metadata['source']} (Page {doc.metadata['page'] + 1})\" for doc in docs)\n",
    "    # Added 1 to the page number assuming 'page' starts at 0 and we want to present it in a user-friendly way\n",
    "\n",
    "    return f\"Documents:\\n{formatted_docs}\\n\\nSources and Pages:\\n{sources_pages}\"\n",
    "\n",
    "# Create a RAG chain using the formatted documents as the context\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs_with_sources(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a parallel chain for retrieving and generating answers\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9243e34a",
   "metadata": {},
   "source": [
    "# 4. Generate Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ceedb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output():\n",
    "    # Prompt the user for a question on ASOP\n",
    "    usr_input = input(\"What is your question on ASOP?: \")\n",
    "\n",
    "    # Invoke the RAG chain with the user input as the question\n",
    "    output = rag_chain_with_source.invoke(usr_input)\n",
    "\n",
    "    # Generate the Markdown output with the question, answer, and context\n",
    "    markdown_output = \"### Question\\n{}\\n\\n### Answer\\n{}\\n\\n### Context\\n\".format(output['question'], output['answer'])\n",
    "\n",
    "    last_page_content = None  # Variable to store the last page content\n",
    "    i = 1 # Source indicator\n",
    "\n",
    "    # Iterate over the context documents to format and include them in the output\n",
    "    for doc in output['context']:\n",
    "        current_page_content = doc.page_content.replace('\\n', '  \\n')  # Get the current page content\n",
    "        \n",
    "        # Check if the current content is different from the last one\n",
    "        if current_page_content != last_page_content:\n",
    "            markdown_output += \"- **Source {}**: {}, page {}:\\n\\n{}\\n\".format(i, doc.metadata['source'], doc.metadata['page'], current_page_content)\n",
    "            i = i + 1\n",
    "        last_page_content = current_page_content  # Update the last page content\n",
    "    \n",
    "    # Display the Markdown output\n",
    "    display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54daa6",
   "metadata": {},
   "source": [
    "### Example questions related to ASOPs\n",
    "- explain ASOP No. 14\n",
    "- How are expenses relfected in cash flow testing based on ASOP No. 22?\n",
    "- What is catastrophe risk?\n",
    "- When do I update assumptions?\n",
    "- What should I do when I do not have credible data to develop non-economic assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36183436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is your question on ASOP?:  explain ASOP No. 14\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Question\n",
       "explain ASOP No. 14\n",
       "\n",
       "### Answer\n",
       " ASOP No. 14, which was related to cash flow testing, has been repealed and its relevant parts were incorporated into ASOP No. 7 and ASOP No. 22 in their revisions. The Actuarial Standards Board (ASB) voted to adopt these revised standards and defer the effective date of ASOP No. 7 while addressing concerns related to property/casualty practice. ASOPs, or Actuarial Standards of Practice, provide guidelines for appropriate actuarial practices in the United States. The repealed ASOP No. 14's topics have been covered by other existing standards and professional conduct codes.\n",
       "\n",
       "### Context\n",
       "- **Source 1**: ../data/ASOP/asop007_128.pdf, page 5:\n",
       "\n",
       "virelated to cash flow testing. Finally, the ASB has adopted a new format for standards, and this   \n",
       "standard has been rewritten to conform to that new format.  In addition to ASOP No. 7, as part of the project  to look at all cash flow testing standards of   \n",
       "practice, ASOP No. 14 and ASOP No. 22 were al so reviewed. Relevant portions of ASOP No.   \n",
       "14 were incorporated within the 2001 revisions of ASOP No. 7 and ASOP No. 22.   At its September 2001 meeting, the ASB voted to adopt the revised ASOP No. 7 and ASOP No. 22 and to repeal ASOP No. 14. In April 2002, the ASB voted to defer the effective date of ASOP No. 7 to July 15, 2002 while it reviewed concerns raised by the Academy’s Casualty Practice Council regarding the standard’s applicability to property/casualty practice. At its June 2002 meeting, the ASB amended the scope to conform to generally accepted casualty actuarial practice. Please see appendix 3 for further information.   Exposure Draft\n",
       "- **Source 2**: ../data/ASOP/asop004_173.pdf, page 31:\n",
       "\n",
       "are found in the current version of ASOP No. 4. The reviewers believe the reference to Precept 8   \n",
       "remains appropriate. The reviewers do not believe that the proposed change significantly improves   \n",
       "the language included in the current version of ASOP No. 4, and made no change.\n",
       "- **Source 3**: ../data/ASOP/asop009_105.pdf, page 2:\n",
       "\n",
       "that the topics in ASOP No. 9 are adequately covered in ASOP No. 41, other ASOPs, and the Code of Professional Conduct , and concluded that ASOP No. 9 should be repealed.    \n",
       " Exposure Draft  \n",
       "   \n",
       " The exposure draft of this repeal document was issued in June 2007 with a comment deadline of   \n",
       "August 15, 2007. Seven comment letters were receive d and were considered in finalizing this   \n",
       "repeal document. For a summary of the substan tive issues and the reviewers’ responses, please   \n",
       "see appendix 2.\n",
       "- **Source 4**: ../data/ASOP/asop017_192.pdf, page 3:\n",
       "\n",
       "ASOP No. 17—Doc. No. 192   \n",
       "   \n",
       "iv   \n",
       " ASOP No. 17 Task Force   \n",
       "          \n",
       "   \n",
       "David R. Godofsky, Chairperson   \n",
       " James P. Galasso   Lawrence J. Sher  Carl M. Harris    Margaret Tiller Sherwood  Adam Reese    \n",
       "   \n",
       "   \n",
       "General Committee of the ASB   \n",
       "   \n",
       "Margaret Tiller Sherwood, Chairperson   \n",
       "Shawna S. Ackerman    Susan E. Pantely    \n",
       "Ralph S. Blanchard III   Judy K. Stromback     \n",
       "Andrew M. Erman  \n",
       "    Hal Tepfer   \n",
       "Dale S. Hagstrom   Christian J. Wolfe       \n",
       "Actuarial Standards Board   \n",
       "   \n",
       "Beth E. Fitzgerald, Chairperson   \n",
       "Christopher S. Carlson  Darrell D. Knapp  Maryellen J. Coggins   Cande J. Olsen Robert M. Damler   Kathleen A. Riley  Mita D. Drazilov   Barbara L. Snyder                    \n",
       "   \n",
       "The Actuarial Standards Board (ASB) sets standards for appropriate actuarial practice in the United   \n",
       "States through the development and promulgation of Actuarial Standards of Practice (ASOPs). These   \n",
       "ASOPs describe the procedures an actuary should follow when performing actuarial services and\n",
       "- **Source 5**: ../data/ASOP/asop011_199.pdf, page 30:\n",
       "\n",
       "after ASOP No. 11 was initially exposed.    \n",
       "Section 3.14, Reliance on Experts (now section 3.16, Reliance on the Expertise of Others)    \n",
       "Comment   \n",
       "   \n",
       "   \n",
       "Response  One commentator said that this section appears to have been drawn from ASOP No. 56, and   \n",
       "suggested deleting duplicative language and adding a reference to ASOP No. 56 instead.   \n",
       "   \n",
       "The reviewers believe the guidance is  not limited to modeling and made no change.\n",
       "- **Source 6**: ../data/ASOP/asop024_184.pdf, page 0:\n",
       "\n",
       "Actuarial Standard    \n",
       "of Practice    \n",
       "No. 24   \n",
       "   \n",
       "   \n",
       "   \n",
       "Compliance with the    \n",
       "NAIC Life Insurance Illustrations    \n",
       "Model Regulation   \n",
       "   \n",
       "   \n",
       "Revised Edition    \n",
       "    \n",
       "Developed by the   \n",
       "Task Force to Revise ASOP No. 24 of the   \n",
       "Life Committee of the   \n",
       "Actuarial Standards Board   \n",
       "    \n",
       "Adopted by the   \n",
       "Actuarial Standards Board   \n",
       "December 2016   \n",
       "   \n",
       "   \n",
       "Doc. No. 184\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5938771",
   "metadata": {},
   "source": [
    "# 5. References\n",
    "- https://www.actuarialstandardsboard.org/standards-of-practice/\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/quickstart\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/sources\n",
    "- https://chat.langchain.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd6605-deab-4be4-afaf-563098577bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
