{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57226901",
   "metadata": {},
   "source": [
    "# Actuarial Standards of Practice (ASOP) Q&A Machine using Retrieval Augmented Generation (RAG)\n",
    "This project aims to create a Retrieval-Augmented Generation (RAG) process for actuaries to ask questions on a set of Actuarial Standards of Practice (ASOP) documents. The RAG process utilizes the power of the Large Language Model (LLM) to provide answers to questions on ASOPs.\n",
    "\n",
    "However, RAG is not without challenges, i.e., hallucination and inaccuracy. This code allows verifiability by providing the context it used to arrive at those answers. This process enables actuaries to validate the information provided by the LLM, empowering them to make informed decisions. By combining the capabilities of LLM with verifiability, this code offers actuaries a robust tool to leverage LLM technology effectively and extract maximum value.\n",
    "\n",
    "The current example uses either OpenAI's GPT 3.5 turbo or a local LLM. Using a local LLM can address potential data privacy or security concerns.\n",
    "\n",
    "View license or further information about the local models used:\n",
    "- Solar 10.7B Instruct: [cc-by-nc-4.0](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0) (non-commercial use)\n",
    "- Mistral 7B Instruct: [Apache License 2.0](https://ollama.com/library/mistral/blobs/sha256:43070e2d4e532684de521b885f385d0841030efa2b1a20bafb76133a5e1379c1)\n",
    "- [GPT4All embedding model](https://python.langchain.com/docs/integrations/text_embedding/gpt4all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c8d88",
   "metadata": {},
   "source": [
    "# 1. Initial Setup\n",
    "This setup includes loading environment variables from a `.env` file, setting the required environment variables, and importing the necessary modules for further processing. It ensures that the code has access to the required APIs and functions for the subsequent tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fc759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial set up\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the variables from .env file and set the API key (or user may manually set the API key)\n",
    "load_dotenv()  # This loads the variables from .env (not part of repo)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" # use when you want to debug or monitor the performance of your langchain applications\n",
    "#os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGCHAIN_API_KEY') # use when accessing cloud-based language models or services that langchain integrates with\n",
    "\n",
    "# Import the necessary modules\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel # for RAG with source\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import glob\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35630ca8-a707-4445-b8a2-661fe3312d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "# Change to False if using local models instead of OpenAI models\n",
    "use_OpenAI = False\n",
    "\n",
    "if use_OpenAI:\n",
    "    embeddings_model = OpenAIEmbeddings()\n",
    "    db_directory = \"../data/chroma_db1\"\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", \n",
    "                     temperature=0) # context window size 16k for GPT 3.5 Turbo\n",
    "\n",
    "else: #Open source models used here are for illustration and educational purposes\n",
    "    embeddings_model = GPT4AllEmbeddings()\n",
    "    db_directory = \"../data/chroma_db2\"\n",
    "    # define a local large language model for the augmented generation\n",
    "    # Ollama is one way to easily run inference\n",
    "    #llm = Ollama(model=\"solar:10.7b-instruct-v1-q5_K_M\")\n",
    "    llm = Ollama(model=\"mistral:instruct\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04673e4",
   "metadata": {},
   "source": [
    "# 2. Load PDF Files and Convert to a Vector DB\n",
    "1. Create a function to load and extract text from PDF files in a specified folder. It defines a function called `load_pdfs_from_folder()` that takes a folder path as input and returns a list of extracted text documents from the PDF files in that folder.\n",
    "\n",
    "2. In the example, the folder path `../data/ASOP` is used, but you can modify it to point to your desired folder.\n",
    "\n",
    "3. By calling the `load_pdfs_from_folder()` function with the folder path, the code loads the PDF files, extracts the text using the PyPDFLoader, and stores the extracted text documents in the `docs` list.\n",
    "\n",
    "4. After loading and extracting the text, a `RecursiveCharacterTextSplitter` object is created with specific parameters for chunking the documents. The `split_documents()` method is then used to split the documents into smaller chunks based on the specified parameters.\n",
    "\n",
    "5. Finally, a Chroma vectorstore is created from the document splits. The vectorstore uses the defined embedding model for embedding the chunks and is saved to the predefined directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3b91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only when the DB directory is empty\n",
    "if not os.path.exists(db_directory) or not os.listdir(db_directory):\n",
    "    # Define a function to load and extract text from PDFs in a folder\n",
    "    def load_pdfs_from_folder(folder_path):\n",
    "        # Get a list of PDF files in the specified folder\n",
    "        pdf_files = glob.glob(f\"{folder_path}/*.pdf\")\n",
    "        docs = []\n",
    "        for pdf_file in pdf_files:\n",
    "            # Load the PDF file using the PyPDFLoader\n",
    "            loader = PyPDFLoader(pdf_file) \n",
    "            # Extract the text from the PDF and add it to the docs list\n",
    "            docs.extend(loader.load())\n",
    "        return docs\n",
    "    \n",
    "    # Example folder path\n",
    "    folder_path = '../data/ASOP'\n",
    "    \n",
    "    # Call the function to load and extract text from PDFs in the specified folder\n",
    "    docs = load_pdfs_from_folder(folder_path)\n",
    "    \n",
    "    # Create a text splitter object with specified parameters\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=200,\n",
    "        length_function=len,)\n",
    "    \n",
    "    # Split the documents into chunks using the text splitter\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # Create a Chroma vector database from the document splits, using OpenAIEmbeddings for embedding\n",
    "    vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                        embedding=embeddings_model, \n",
    "                                        persist_directory=db_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46105ba",
   "metadata": {},
   "source": [
    "# 3. Retrieve from the Vector DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9abd0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Chroma vector database with specified parameters\n",
    "vectorstore = Chroma(embedding_function=embeddings_model, \n",
    "                     persist_directory=db_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9744b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve and RAG chain\n",
    "\n",
    "# Create a retriever using the vector database as the search source\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", \n",
    "                                     search_kwargs={'k': 6, 'lambda_mult': 0.25}) \n",
    "# Use MMR (Maximum Marginal Relevance) to find a set of documents that are both similar to the input query and diverse among themselves\n",
    "# Increase the number of documents to get, and increase diversity (lambda mult 0.5 being default, 0 being the most diverse, 1 being the least)\n",
    "\n",
    "# Load the RAG (Retrieval-Augmented Generation) prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define a function to format the documents with their sources and pages\n",
    "def format_docs_with_sources(docs):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    sources_pages = \"\\n\".join(f\"{doc.metadata['source']} (Page {doc.metadata['page'] + 1})\" for doc in docs)\n",
    "    # Added 1 to the page number assuming 'page' starts at 0 and we want to present it in a user-friendly way\n",
    "\n",
    "    return f\"Documents:\\n{formatted_docs}\\n\\nSources and Pages:\\n{sources_pages}\"\n",
    "\n",
    "# Create a RAG chain using the formatted documents as the context\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs_with_sources(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a parallel chain for retrieving and generating answers\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9243e34a",
   "metadata": {},
   "source": [
    "# 4. Generate Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ceedb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output():\n",
    "    # Prompt the user for a question on ASOP\n",
    "    usr_input = input(\"What is your question on ASOP?: \")\n",
    "\n",
    "    # Invoke the RAG chain with the user input as the question\n",
    "    output = rag_chain_with_source.invoke(usr_input)\n",
    "\n",
    "    # Generate the Markdown output with the question, answer, and context\n",
    "    markdown_output = \"### Question\\n{}\\n\\n### Answer\\n{}\\n\\n### Context\\n\".format(output['question'], output['answer'])\n",
    "\n",
    "    last_page_content = None  # Variable to store the last page content\n",
    "    i = 1 # Source indicator\n",
    "\n",
    "    # Iterate over the context documents to format and include them in the output\n",
    "    for doc in output['context']:\n",
    "        current_page_content = doc.page_content.replace('\\n', '  \\n')  # Get the current page content\n",
    "        \n",
    "        # Check if the current content is different from the last one\n",
    "        if current_page_content != last_page_content:\n",
    "            markdown_output += \"- **Source {}**: {}, page {}:\\n\\n{}\\n\".format(i, doc.metadata['source'], doc.metadata['page'], current_page_content)\n",
    "            i = i + 1\n",
    "        last_page_content = current_page_content  # Update the last page content\n",
    "    \n",
    "    # Display the Markdown output\n",
    "    display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54daa6",
   "metadata": {},
   "source": [
    "### Example questions related to ASOPs\n",
    "- explain ASOP No. 14\n",
    "- How are expenses relfected in cash flow testing based on ASOP No. 22?\n",
    "- What is catastrophe risk?\n",
    "- When do I update assumptions?\n",
    "- What should I do when I do not have credible data to develop non-economic assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36183436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is your question on ASOP?:  explain ASOP No. 14\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Question\n",
       "explain ASOP No. 14\n",
       "\n",
       "### Answer\n",
       " ASOP No. 14 originally provided guidance for cash flow testing in life and health insurance companies. In 2001, relevant portions of this guidance were incorporated into ASOPs No. 7 and No. 22, leading to the repeal of ASOP No. 14 for work performed on or after April 15, 2002. The reviewers suggested addressing duplicative language by clarifying the meaning of \"deviation\" in relation to ASOPs and maintaining consistency across ASOPs.\n",
       "\n",
       "### Context\n",
       "- **Source 1**: ../data/ASOP/asop011_199.pdf, page 30:\n",
       "\n",
       "after ASOP No. 11 was initially exposed.    \n",
       "Section 3.14, Reliance on Experts (now section 3.16, Reliance on the Expertise of Others)    \n",
       "Comment   \n",
       "   \n",
       "   \n",
       "Response  One commentator said that this section appears to have been drawn from ASOP No. 56, and   \n",
       "suggested deleting duplicative language and adding a reference to ASOP No. 56 instead.   \n",
       "   \n",
       "The reviewers believe the guidance is  not limited to modeling and made no change.\n",
       "- **Source 2**: ../data/ASOP/asop041_120.pdf, page 26:\n",
       "\n",
       "address the concerns of the commenta tor. The revised section 4.4 is part   \n",
       "of the ASB initiative to move all su bstantive guidance on deviation into   \n",
       "ASOP No. 41 (and thus achieve consis tency across ASOPs.) Part of this   \n",
       "initiative is to clarify that “devia tion” means deviating from the guidance   \n",
       "of an ASOP. Compliance with th e ASOP is still possible through   \n",
       "adequate disclosure.\n",
       "- **Source 3**: ../data/ASOP/asop051_188.pdf, page 5:\n",
       "\n",
       "ASOP No. 51—September 2017   \n",
       "   \n",
       "   \n",
       " viThe Pension Committee thanks former committee chairperson Mita D. Drazilov and former   \n",
       "committee members Fiona E. Liston, Mitchell I. Serota, Judy K. Stromback, and Virginia C.   \n",
       "Wentz for their assistance with drafting this ASOP.  The ASB voted in September 2017 to adopt this standard.\n",
       "- **Source 4**: ../data/ASOP/asop001_170.pdf, page 20:\n",
       "\n",
       "the ASOP, because it doesn’t tell  the actuary to do anything.    \n",
       "   \n",
       "Failure to comply with the ASOPs results in a breach of the Code. The reviewers   \n",
       "believe this is an important point that  belongs in the body of the Introductory   \n",
       "ASOP. Therefore, no change was made.\n",
       "- **Source 5**: ../data/ASOP/asop014_082.pdf, page 1:\n",
       "\n",
       "and Health Insurance Companies , to provide guidance in determining whether or not   \n",
       "to do cash flow testing in forming a professional opinion or recommendation.    \n",
       "   \n",
       "As part of the project to look at all cash flow testing standards of p ractice, a task force   \n",
       "of the ASB’s Life Committee reviewed ASOP No. 7 (titled, as of September 2001,   \n",
       "Analysis of Life, Health, or Property/Casualty Insurer Cash Flows ), ASOP No. 14   \n",
       "(When to do Cash Flow Testing for Life and Health Insurance Companies ), and    \n",
       "ASOP No. 22 (titled, as of September 2001, Statements of Opinion Based on Asset   \n",
       "Adequacy Analysis by Actuaries for Life or Health Insurers ). Relevant portions of   \n",
       "ASOP No. 14 were incorporated within the 2001 revisions of ASOP No. 7 and   \n",
       "ASOP No. 22.    \n",
       "   \n",
       "At its September 2001 meeting, the ASB voted to adopt the revised ASOP No. 7 and   \n",
       "ASOP No. 22 and to repeal ASOP No. 14.    \n",
       "   \n",
       "ASOP No. 14 is repealed for any work performed on or after April 15, 2002.\n",
       "- **Source 6**: ../data/ASOP/asop038_201.pdf, page 3:\n",
       "\n",
       "2019. Changes have been made to this exposure draft of ASOP No. 38 to be consistent with ASOP   \n",
       "No. 56 and other recent ASOPs.   \n",
       "   \n",
       "The exposure draft of this revision of ASOP No. 38 was the work of the Catastrophe Modeling   \n",
       "Task Force, whose membership has experience in life insurance, health insurance,   \n",
       "property/casualty insurance, and enterprise risk management.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5938771",
   "metadata": {},
   "source": [
    "# 5. References\n",
    "- https://www.actuarialstandardsboard.org/standards-of-practice/\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/quickstart\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/sources\n",
    "- https://python.langchain.com/docs/integrations/text_embedding/\n",
    "- https://docs.gpt4all.io/gpt4all_python_embedding.html#gpt4all.gpt4all.Embed4All\n",
    "- https://chat.langchain.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
