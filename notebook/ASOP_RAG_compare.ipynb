{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57226901",
   "metadata": {},
   "source": [
    "# Actuarial Standards of Practice (ASOP) Q&A Machine using Retrieval Augmented Generation (RAG)\n",
    "This project aims to create a Retrieval-Augmented Generation (RAG) process for actuaries to ask questions on a set of Actuarial Standards of Practice (ASOP) documents. The RAG process utilizes the power of the Large Language Model (LLM) to provide answers to questions on ASOPs.\n",
    "\n",
    "However, RAG is not without challenges, i.e., hallucination and inaccuracy. This code allows verifiability by providing the context it used to arrive at those answers. This process enables actuaries to validate the information provided by the LLM, empowering them to make informed decisions. By combining the capabilities of LLM with verifiability, this code offers actuaries a robust tool to leverage LLM technology effectively and extract maximum value.\n",
    "\n",
    "The current example uses either OpenAI's GPT 3.5 turbo AND a local LLM. Note that the current notebook is set up to output results from both models for comparison purposes.  \n",
    "Using a local LLM can address potential data privacy or security concerns.\n",
    "\n",
    "View license or further information about the local models used:\n",
    "- Solar 10.7B Instruct: [cc-by-nc-4.0](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0) (non-commercial use)\n",
    "- Mistral 7B Instruct: [Apache License 2.0](https://ollama.com/library/mistral/blobs/sha256:43070e2d4e532684de521b885f385d0841030efa2b1a20bafb76133a5e1379c1)\n",
    "- [GPT4All embedding model](https://python.langchain.com/docs/integrations/text_embedding/gpt4all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c8d88",
   "metadata": {},
   "source": [
    "# 1. Initial Setup\n",
    "This setup includes loading environment variables from a `.env` file, setting the required environment variables, and importing the necessary modules for further processing. It ensures that the code has access to the required APIs and functions for the subsequent tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fc759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial set up\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the variables from .env file and set the API key (or user may manually set the API key)\n",
    "load_dotenv()  # This loads the variables from .env (not part of repo)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" # use when you want to debug or monitor the performance of your langchain applications\n",
    "#os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGCHAIN_API_KEY') # use when accessing cloud-based language models or services that langchain integrates with\n",
    "\n",
    "# Import the necessary modules\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel # for RAG with source\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import glob\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35630ca8-a707-4445-b8a2-661fe3312d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "# use_OpenAI\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "db_directory = \"../data/chroma_db1\"\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", \n",
    "                 temperature=0) # context window size 16k for GPT 3.5 Turbo\n",
    "\n",
    "# Open source models used here are for illustration and educational purposes\n",
    "# Added _ALT at the end of variables to indicate alternative method as a comparison\n",
    "embeddings_model_ALT = GPT4AllEmbeddings()\n",
    "db_directory_ALT = \"../data/chroma_db2\"\n",
    "# define a local large language model for the augmented generation\n",
    "# Ollama is one way to easily run inference\n",
    "#llm = Ollama(model=\"solar:10.7b-instruct-v1-q5_K_M\")\n",
    "llm_ALT = Ollama(model=\"mistral:instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04673e4",
   "metadata": {},
   "source": [
    "# 2. Load PDF Files and Convert to a Vector DB\n",
    "1. Create a function to load and extract text from PDF files in a specified folder. It defines a function called `load_pdfs_from_folder()` that takes a folder path as input and returns a list of extracted text documents from the PDF files in that folder.\n",
    "\n",
    "2. In the example, the folder path `../data/ASOP` is used, but you can modify it to point to your desired folder.\n",
    "\n",
    "3. By calling the `load_pdfs_from_folder()` function with the folder path, the code loads the PDF files, extracts the text using the PyPDFLoader, and stores the extracted text documents in the `docs` list.\n",
    "\n",
    "4. After loading and extracting the text, a `RecursiveCharacterTextSplitter` object is created with specific parameters for chunking the documents. The `split_documents()` method is then used to split the documents into smaller chunks based on the specified parameters.\n",
    "\n",
    "5. Finally, a Chroma vectorstore is created from the document splits. The vectorstore uses the defined embedding model for embedding the chunks and is saved to the predefined directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3b91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only when the DB directory is empty\n",
    "if not os.path.exists(db_directory) or not os.listdir(db_directory):\n",
    "    # Define a function to load and extract text from PDFs in a folder\n",
    "    def load_pdfs_from_folder(folder_path):\n",
    "        # Get a list of PDF files in the specified folder\n",
    "        pdf_files = glob.glob(f\"{folder_path}/*.pdf\")\n",
    "        docs = []\n",
    "        for pdf_file in pdf_files:\n",
    "            # Load the PDF file using the PyPDFLoader\n",
    "            loader = PyPDFLoader(pdf_file) \n",
    "            # Extract the text from the PDF and add it to the docs list\n",
    "            docs.extend(loader.load())\n",
    "        return docs\n",
    "    \n",
    "    # Example folder path\n",
    "    folder_path = '../data/ASOP'\n",
    "    \n",
    "    # Call the function to load and extract text from PDFs in the specified folder\n",
    "    docs = load_pdfs_from_folder(folder_path)\n",
    "    \n",
    "    # Create a text splitter object with specified parameters\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=200,\n",
    "        length_function=len,)\n",
    "    \n",
    "    # Split the documents into chunks using the text splitter\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # Create a Chroma vector database from the document splits, using OpenAIEmbeddings for embedding\n",
    "    vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                        embedding=embeddings_model, \n",
    "                                        persist_directory=db_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46105ba",
   "metadata": {},
   "source": [
    "# 3. Retrieve from the Vector DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9abd0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Chroma vector database with specified parameters\n",
    "vectorstore = Chroma(embedding_function=embeddings_model, \n",
    "                     persist_directory=db_directory)\n",
    "\n",
    "## Alternative method using local open-source LLM\n",
    "vectorstore_ALT = Chroma(embedding_function=embeddings_model_ALT, \n",
    "                     persist_directory=db_directory_ALT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9744b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve and RAG chain\n",
    "\n",
    "# Create a retriever using the vector database as the search source\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", \n",
    "                                     search_kwargs={'k': 6, 'lambda_mult': 0.25})\n",
    "\n",
    "## Alternative method using local open-source LLM\n",
    "retriever_ALT = vectorstore_ALT.as_retriever(search_type=\"mmr\", \n",
    "                                     search_kwargs={'k': 6, 'lambda_mult': 0.25})\n",
    "\n",
    "# Use MMR (Maximum Marginal Relevance) to find a set of documents that are both similar to the input query and diverse among themselves\n",
    "# Increase the number of documents to get, and increase diversity (lambda mult 0.5 being default, 0 being the most diverse, 1 being the least)\n",
    "\n",
    "# Load the RAG (Retrieval-Augmented Generation) prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define a function to format the documents with their sources and pages\n",
    "def format_docs_with_sources(docs):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    sources_pages = \"\\n\".join(f\"{doc.metadata['source']} (Page {doc.metadata['page'] + 1})\" for doc in docs)\n",
    "    # Added 1 to the page number assuming 'page' starts at 0 and we want to present it in a user-friendly way\n",
    "\n",
    "    return f\"Documents:\\n{formatted_docs}\\n\\nSources and Pages:\\n{sources_pages}\"\n",
    "\n",
    "# Create a RAG chain using the formatted documents as the context\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs_with_sources(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a parallel chain for retrieving and generating answers\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)\n",
    "\n",
    "## Alternative method using local open-source LLM\n",
    "# Create a RAG chain using the formatted documents as the context\n",
    "rag_chain_from_docs_ALT = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs_with_sources(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm_ALT\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a parallel chain for retrieving and generating answers\n",
    "rag_chain_with_source_ALT = RunnableParallel(\n",
    "    {\"context\": retriever_ALT, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs_ALT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9243e34a",
   "metadata": {},
   "source": [
    "# 4. Generate Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ceedb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output():\n",
    "    # Prompt the user for a question on ASOP\n",
    "    usr_input = input(\"What is your question on ASOP?: \")\n",
    "\n",
    "    # Invoke the RAG chain with the user input as the question\n",
    "    output = rag_chain_with_source.invoke(usr_input)\n",
    "    output_ALT = rag_chain_with_source_ALT.invoke(usr_input)\n",
    "\n",
    "    # Generate the Markdown output with the question, answer, and context\n",
    "    markdown_output = \"### Question\\n{}\\n\\n### Open AI Answer\\n{}\\n\\n\".format(output['question'], output['answer'])\n",
    "    markdown_output += \"### Local LLM Answer\\n{}\\n\\n### Open AI Context\\n\".format(output_ALT['answer'])\n",
    "\n",
    "    last_page_content = None  # Variable to store the last page content\n",
    "    i = 1 # Source indicator\n",
    "\n",
    "    # Iterate over the context documents to format and include them in the output\n",
    "    for doc in output['context']:\n",
    "        current_page_content = doc.page_content.replace('\\n', '  \\n')  # Get the current page content\n",
    "        \n",
    "        # Check if the current content is different from the last one\n",
    "        if current_page_content != last_page_content:\n",
    "            markdown_output += \"- **Open AI Source {}**: {}, page {}:\\n\\n{}\\n\".format(i, doc.metadata['source'], doc.metadata['page'], current_page_content)\n",
    "            i = i + 1\n",
    "        last_page_content = current_page_content  # Update the last page content\n",
    "\n",
    "    markdown_output += \"\\n\\n### Local LLM Context\\n\"\n",
    "\n",
    "    last_page_content = None  # Variable to store the last page content\n",
    "    i = 1 # Source indicator\n",
    "\n",
    "    # Iterate over the context documents to format and include them in the output\n",
    "    for doc in output_ALT['context']:\n",
    "        current_page_content = doc.page_content.replace('\\n', '  \\n')  # Get the current page content\n",
    "        \n",
    "        # Check if the current content is different from the last one\n",
    "        if current_page_content != last_page_content:\n",
    "            markdown_output += \"- **Local LLM Source {}**: {}, page {}:\\n\\n{}\\n\".format(i, doc.metadata['source'], doc.metadata['page'], current_page_content)\n",
    "            i = i + 1\n",
    "        last_page_content = current_page_content  # Update the last page content\n",
    "\n",
    "    \n",
    "    # Display the Markdown output\n",
    "    display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54daa6",
   "metadata": {},
   "source": [
    "### Example questions related to ASOPs\n",
    "- explain ASOP No. 14\n",
    "- How are expenses relfected in cash flow testing based on ASOP No. 22?\n",
    "- What is catastrophe risk?\n",
    "- When do I update assumptions?\n",
    "- What should I do when I do not have credible data to develop non-economic assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36183436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is your question on ASOP?:  What should I do when I do not have credible data to develop non-economic assumptions?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Question\n",
       "What should I do when I do not have credible data to develop non-economic assumptions?\n",
       "\n",
       "### Open AI Answer\n",
       "When credible data is not available to develop non-economic assumptions, consider other information sources such as pricing practices in the insurance business or experience of other insurance companies. Obtain input from individuals with relevant expertise and give weight to their input when developing assumptions. If necessary, make judgmental adjustments or assumptions to the data, disclose them, and consider the potential uncertainty or bias in the results.\n",
       "\n",
       "### Local LLM Answer\n",
       " When you don't have credible data to develop non-economic assumptions, consider the following steps based on the provided context from ASOP No. 23 and ASOP No. 25:\n",
       "\n",
       "1. Review the overall data quality to determine if it is sufficient for completing the assignment (ASOP No. 23). If necessary, make assumptions based on relevant available data, such as averages or industry benchmarks.\n",
       "2. Ensure consistency among all selected assumptions for a particular measurement and assess their appropriateness in the current model run (ASOP No. 23 and ASOP No. 056).\n",
       "3. Assess the reasonability of the model output when determining if assumptions are reasonable in aggregate, recognizing the legitimacy of benefit plan experience (ASOP No. 25).\n",
       "4. Evaluate data quality at each level of usage as you combine or separate data for your analysis (ASOP No. 23).\n",
       "5. Consider alternative data sources and elements, taking into account their availability and relevance to the purpose of your analysis (ASOP No. 23).\n",
       "\n",
       "### Open AI Context\n",
       "- **Open AI Source 1**: ../data/ASOP/asop019_137.pdf, page 7:\n",
       "\n",
       "information is available. When experience of the business is unavailable or insufficient to   \n",
       "provide a credible basis on which to develo p assumptions, the actuary should consider   \n",
       "other information sources in setting assumptions. Other information sources may include the pricing or reserving practices applicable  to the insurance business and the available   \n",
       "experience of other insurance businesses with comparable policies or contracts, markets,   \n",
       "and operating environment.    \n",
       "  In developing assumptions for which the actuary believes additional expertise is needed,   \n",
       "the actuary should obtain necessary input from persons possessing the relevant   \n",
       "knowledge or expertise, and should gi ve due weight to their input.    \n",
       "  When setting assumptions for use in an appraisal, the actuary should take reasonable   \n",
       "steps to ensure that each set of assump tions used is internally consistent.   \n",
       "   \n",
       "3.4 Discount Rate  \n",
       "⎯If the appraisal is based on the di scounted value of projected earnings,\n",
       "- **Open AI Source 2**: ../data/ASOP/asop027_197.pdf, page 20:\n",
       "\n",
       "study. For each previously selected assumption that the actuary determines is no longer   \n",
       "reasonable, the actuary should select a reasonable new assumption.    \n",
       "   \n",
       "3.14 Assessing Assumptions Not Selected by the Actuary—At each measurement date , the   \n",
       "actuary should assess the reasonableness of each economic assumption that the actuary has   \n",
       "not selected (other than prescribed assumptions or methods set by law  or assumptions   \n",
       "disclosed in accordance with section 4.2[b]), using the guidance set forth in this standard   \n",
       "to the extent practicable.   \n",
       "   \n",
       "3.15 Phase-In of Changes in Assumptions—If an  economic assumption is being phased in over   \n",
       "a period that includes multiple measurement dates , the actuary should determine the   \n",
       "reasonableness of the  economic assumption and its consistency with other assumptions as   \n",
       "of the measurement date  at which it is applied, without regard to changes to the   \n",
       "assumption planned for future measurement dates . If the actuary determines that an\n",
       "- **Open AI Source 3**: ../data/ASOP/asop013_133.pdf, page 8:\n",
       "\n",
       "for most property/casualty insuranc e plans or policies. In such procedures, actuaries generally   \n",
       "place reliance on (1) data generated by the book of  business being analyzed, (2) other insurance   \n",
       "data, and (3) non-insurance data, in that order of preference.  Mathematical techniques are often   \n",
       "used to smooth and extrapolate from historical data. In the absence of strong contrary   \n",
       "indications, there is a reliance on extrapolations of historical in surance data. Procedures based on   \n",
       "non-insurance data are also use d. In trending procedures, judgmen tal considerations generally   \n",
       "include, but are not limited to, th e historical data used, the succe ss of these techniques in making   \n",
       "prior projections, the statistical goodness of fit of the techniques to the historical data, and the   \n",
       "impact of any sudden, nonrecurring changes (for example, tort re form) which had not yet been   \n",
       "incorporated in the historical data.\n",
       "- **Open AI Source 4**: ../data/ASOP/asop023_185.pdf, page 8:\n",
       "\n",
       "obtain additional or corrected data  that will allow the analysis to be performed;   \n",
       "   \n",
       "c. judgmental adjustments or assu mptions can be applied to the data  that allow the   \n",
       "actuary to perform the analysis. Any judgmental adjustments to data  or assumptions   \n",
       "should be disclosed in acco rdance with section 4.1(f). If the actuary judges that the   \n",
       "use of the data , even with adjustments and a ssumptions applied, may cause the   \n",
       "results to be highly uncertain  or contain a significant bias, the actuary may choose to   \n",
       "complete the assignment but should disclose the potential existence of the uncertainty   \n",
       "or bias, and, if reasonably determinable, the nature and potentia l magnitude of such   \n",
       "uncertainty or bias, in accordance with se ction 4.1(g). Alterna tively, the actuary may   \n",
       "compensate for the data  deficiencies by adjusting the results, such as by increasing   \n",
       "the range of reasonable estimates, and disclose the adjustments, in accordance with section 4.1(f);\n",
       "- **Open AI Source 5**: ../data/ASOP/asop051_188.pdf, page 24:\n",
       "\n",
       "ASOP No. 51—September 2017   \n",
       "   \n",
       "   \n",
       " 18Section 3.4, Assumptions for Assess ment of Risk (now  section 3.5)   \n",
       "Comment   \n",
       "   \n",
       "   \n",
       "Response One commentator suggested that empirical data be used to select assumptions rather than using   \n",
       "professional judgment.   \n",
       "   \n",
       "The reviewers note that the section Assumptions for Assessment of Risk reads “The assumptions used for assessment of risk may be based on economic and demographic data and analyses,” but   \n",
       "believe “the actuary should use professional judgment in selecting [these] assumptions” and made   \n",
       "no change in response to this comment.   \n",
       "Comment    \n",
       "   \n",
       "Response Two commentators suggested the term “plausible” is  not clear and also that implausible outcomes   \n",
       "should be considered.   \n",
       "   \n",
       "The reviewers believe the term “plausible,” combined with the requirement for the actuary to use professional judgment, is appropriate for this standard and made no change in response to this   \n",
       "comment.   \n",
       "Comment\n",
       "- **Open AI Source 6**: ../data/ASOP/asop046_165.pdf, page 12:\n",
       "\n",
       "ASOP No. 46—September 2012    \n",
       "   \n",
       " 7b. prices in the marketplace;   \n",
       "   \n",
       "c. opinions of other experts;   \n",
       "   \n",
       "d. the fit of the assumed dist ribution to available data;   \n",
       "   \n",
       "e. the ability of the assumed distribution to reflect possible extreme values;    \n",
       "   \n",
       "f. sensitivity of results to changes in assumptions;    \n",
       "   \n",
       "g.  internal consistency of the assumptions; and  h. consistency in the app lication of assumptions.   \n",
       " 3.3.5   Validation of the Economic Capital Model  \n",
       "—Economic capital is often   \n",
       "determined based on the resu lts of stochastic models that produce a large number   \n",
       "of outcomes. The actuary should devise a ppropriate tests of the distribution of   \n",
       "outcomes calculated by the model (for example, in comparison to the range of results in similar models or to histori cal outcomes over time) and the sensitivity of   \n",
       "those distributions to changes in the assumptions and parameters. The actuary should also perform validation tests to determine whether the model results are\n",
       "\n",
       "\n",
       "### Local LLM Context\n",
       "- **Local LLM Source 1**: ../data/ASOP/asop035_198.pdf, page 18:\n",
       "\n",
       "elements such as birth dates or hire dates. Accordingly, assumptions for missing or   \n",
       "incomplete data may be necessary if the actuary has determined, in accordance with   \n",
       "ASOP No. 23, Data Quality , that the overall data are of sufficient quality to   \n",
       "complete the assignment. Data actually supplied may be relevant in making such   \n",
       "assumptions. For example, it may be appropriate to assume a missing birth date is   \n",
       "equal to the average birth date for other participants who have complete data and   \n",
       "who have the same service credits as the participant whose date of birth is missing.   \n",
       "   \n",
       "3.6 Consistency among Assumptions Selected by the Actuary for a Particular Measurement—  \n",
       "With respect to a particular measurement, the actuary should select demographic   \n",
       "assumptions  that are consistent with the other assumptions selected by the actuary,   \n",
       "including economic assumptions, unless an assumption considered individually is not\n",
       "- **Local LLM Source 2**: ../data/ASOP/asop056_195.pdf, page 11:\n",
       "\n",
       "identifying the possibility of  an inconsistency with other assumptions .    \n",
       "   \n",
       "d. Appropriateness of Input in Curre nt Model Run—Where practical a nd   \n",
       "appropriate, the actuary reusing an existing model  should evaluate whether   \n",
       "input unchanged from a prior model run is still appropriate  for use in the   \n",
       "current model run . For example, models  used in financial reporting may   \n",
       "offer opportunities to compare assumptions  to emerging experience in the   \n",
       "aggregate.    \n",
       "   \n",
       "e. Reasonable Model in the Aggregate—The actuary should assess the    \n",
       "reasonability of the model output  when determining whether the   \n",
       "assumptions  are reasonable in the aggregate. While assumptions  might\n",
       "- **Local LLM Source 3**: ../data/ASOP/asop006_177.pdf, page 30:\n",
       "\n",
       "plan experience relative to normative  ranges of value but also recognize   \n",
       "the legitimacy of the benefit plan  experience, to the extent it is credible,   \n",
       "and the limitations of applying normative data to an unrelated situation.   \n",
       "ASOP No. 25 provides guidance in the assignment of credibility values to   \n",
       "data.   \n",
       "     c. Data Quality at Each Level of Usage—Data that may be of appropriate   \n",
       "quality for determination of certain assumptions within a model may not   \n",
       "be of appropriate quality for determ ination of other assumptions. When   \n",
       "data are combined or separated, the actuary should review the data for   \n",
       "suitability for the purpose. For example, data from a benefit plan  may be   \n",
       "sufficient for setting an aggregate per ca pita health care cost but not be of   \n",
       "sufficient size to set per capita health care costs by location.    \n",
       "  3.10 Administrative Inconsistencies—In general, the actuary may rely on the plan sponsor’s\n",
       "- **Local LLM Source 4**: ../data/ASOP/asop023_185.pdf, page 7:\n",
       "\n",
       "ASOP No. 23—Doc. No. 185    \n",
       "   \n",
       "3   \n",
       "   \n",
       " sets or data  sources, if any, to be considere d. The actuary should do the following:   \n",
       "   \n",
       "a.        consider the data elements  that are desired and possible alternative data elements ;   \n",
       "and   \n",
       "   \n",
       "b.        select the data  for the analysis with consideration of the following:   \n",
       "   \n",
       "1. whether the data  constitute appropriate data , including whether the data  are   \n",
       "sufficiently current;    \n",
       "   \n",
       "2. whether the data  are reasonable with particular attention to internal   \n",
       "consistency;   \n",
       "   \n",
       "3. whether the data  are reasonable given relevant external information that is   \n",
       "readily available and known to the actuary;   \n",
       "   \n",
       "4. the degree to which the data  are sufficient ;   \n",
       "   \n",
       "5. any known significant limitations of the  data ;   \n",
       "   \n",
       "6. the availability of additional or alternative data  and the benefit to be gained   \n",
       "from such additional or alternative data , balanced against how practical it is to   \n",
       "collect and compile such additional or alternative data ; and\n",
       "- **Local LLM Source 5**: ../data/ASOP/asop035_198.pdf, page 12:\n",
       "\n",
       "rather than selecting a separate assumption for each.   \n",
       "   \n",
       " 3.2.2 Consider the Relevant Assumption Universe—The actuary should be familiar with   \n",
       "the assumption universe  relevant to each type of assumption identified in section   \n",
       "3.2.1. The assumption universe  may include tables or factors particular to the   \n",
       "given plan as well as general tables, factors, and modifications to the tables that are   \n",
       "available to the actuary. Sources of information relevant to demographic   \n",
       "assumptions  may include the following:   \n",
       "   \n",
       "  a. experience studies or published tables based on experience under uninsured   \n",
       "plans and annuity contracts, or based on any other populations considered   \n",
       "representative of the group at hand;   \n",
       "   \n",
       "  b. relevant plan or plan sponsor experience, which may include analyses of   \n",
       "gains or losses by source;    \n",
       "   \n",
       "  c. studies or reports of the effects of plan design, specific events (for example,   \n",
       "shutdown), economic conditions, or sponsor characteristics on the\n",
       "- **Local LLM Source 6**: ../data/ASOP/asop022_203.pdf, page 10:\n",
       "\n",
       "business . For example, mortality improvement may be different   \n",
       "between life and annuity products ;   \n",
       "   \n",
       "b. the source and credibility of the data from which the assumptions    \n",
       "are derived (f or further guidance, the actuary should refer to ASOP   \n",
       "No. 23, Data Quality , and ASOP No. 25, Credibility Procedures ).   \n",
       "For example, different trends may be appropriate when using   \n",
       "company experience vs. industry studies ; and    \n",
       "   \n",
       "c. the impact of trends on cash flows . For example, the effect of   \n",
       "future economic conditions on policyholder elections .   \n",
       "   \n",
       "3.1.2.2 Margins —The actuary should consider including margins in assumptions   \n",
       "to reflect adverse deviation . When determining the level of  assumption   \n",
       "margins , if any, the actuary should take into account the following:    \n",
       "   \n",
       "a. the level of uncertainty for the assumption, including sparsity of   \n",
       "data;\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5938771",
   "metadata": {},
   "source": [
    "# 5. References\n",
    "- https://www.actuarialstandardsboard.org/standards-of-practice/\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/quickstart\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/sources\n",
    "- https://python.langchain.com/docs/integrations/text_embedding/\n",
    "- https://docs.gpt4all.io/gpt4all_python_embedding.html#gpt4all.gpt4all.Embed4All\n",
    "- https://chat.langchain.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
