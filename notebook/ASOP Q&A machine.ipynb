{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57226901",
   "metadata": {},
   "source": [
    "# Actuarial Standards of Practice (ASOP) Q&A Machine using Retrieval Augmented Generation (RAG)\n",
    "This project aims to create a Retrieval-Augmented Generation (RAG) process for actuaries to ask questions on a set of Actuarial Standards of Practice (ASOP) documents. The RAG process utilizes the power of the Large Language Model (LLM) to provide answers to questions on ASOPs.\n",
    "\n",
    "However, RAG is not without challenges, i.e., hallucination and inaccuracy. This code allows verifiability by providing the context it used to arrive at those answers. This process enables actuaries to validate the information provided by the LLM, empowering them to make informed decisions. By combining the capabilities of LLM with verifiability, this code offers actuaries a robust tool to leverage LLM technology effectively and extract maximum value.\n",
    "\n",
    "The current example uses either OpenAI's GPT 3.5 turbo or a local LLM. Using local LLM can address potential data privacy or security concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c8d88",
   "metadata": {},
   "source": [
    "# 1. Initial Setup\n",
    "This setup includes loading environment variables from a `.env` file, setting the required environment variables, and importing the necessary modules for further processing. It ensures that the code has access to the required APIs and functions for the subsequent tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fc759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial set up\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the variables from .env file\n",
    "load_dotenv()  # This loads the variables from .env (not part of repo)\n",
    "\n",
    "# Set the environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGCHAIN_API_KEY')\n",
    "\n",
    "# Import the necessary modules\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel # for RAG with source\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import glob\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04673e4",
   "metadata": {},
   "source": [
    "# 2. Load PDF Files and Convert to a Vector DB\n",
    "1. Create a function to load and extract text from PDF files in a specified folder. It defines a function called `load_pdfs_from_folder()` that takes a folder path as input and returns a list of extracted text documents from the PDF files in that folder.\n",
    "\n",
    "2. In the example, the folder path `../data/ASOP` is used, but you can modify it to point to your desired folder.\n",
    "\n",
    "3. By calling the `load_pdfs_from_folder()` function with the folder path, the code loads the PDF files, extracts the text using the PyPDFLoader, and stores the extracted text documents in the `docs` list.\n",
    "\n",
    "4. After loading and extracting the text, a `RecursiveCharacterTextSplitter` object is created with specific parameters for chunking the documents. The `split_documents()` method is then used to split the documents into smaller chunks based on the specified parameters.\n",
    "\n",
    "5. Finally, a Chroma vectorstore is created from the document splits. The vectorstore uses the `OpenAIEmbeddings` for embedding the chunks and is persisted to the directory `../data/chroma_db1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3b91d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Uncomment when creating your own vector database for the first time\\n# Define a function to load and extract text from PDFs in a folder\\ndef load_pdfs_from_folder(folder_path):\\n    # Get a list of PDF files in the specified folder\\n    pdf_files = glob.glob(f\"{folder_path}/*.pdf\")\\n    docs = []\\n    for pdf_file in pdf_files:\\n        # Load the PDF file using the PyPDFLoader\\n        loader = PyPDFLoader(pdf_file) \\n        # Extract the text from the PDF and add it to the docs list\\n        docs.extend(loader.load())\\n    return docs\\n\\n# Example folder path\\nfolder_path = \\'../data/ASOP\\'\\n\\n# Call the function to load and extract text from PDFs in the specified folder\\ndocs = load_pdfs_from_folder(folder_path)\\n\\n# Create a text splitter object with specified parameters\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000, \\n    chunk_overlap=200,\\n    length_function=len,)\\n\\n# Split the documents into chunks using the text splitter\\nsplits = text_splitter.split_documents(docs)\\n\\n# Create a Chroma vector database from the document splits, using OpenAIEmbeddings for embedding\\nvectorstore = Chroma.from_documents(documents=splits, \\n                                    embedding=OpenAIEmbeddings(), \\n                                    persist_directory=\"../data/chroma_db1\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Uncomment when creating your own vector database for the first time\n",
    "# Define a function to load and extract text from PDFs in a folder\n",
    "def load_pdfs_from_folder(folder_path):\n",
    "    # Get a list of PDF files in the specified folder\n",
    "    pdf_files = glob.glob(f\"{folder_path}/*.pdf\")\n",
    "    docs = []\n",
    "    for pdf_file in pdf_files:\n",
    "        # Load the PDF file using the PyPDFLoader\n",
    "        loader = PyPDFLoader(pdf_file) \n",
    "        # Extract the text from the PDF and add it to the docs list\n",
    "        docs.extend(loader.load())\n",
    "    return docs\n",
    "\n",
    "# Example folder path\n",
    "folder_path = '../data/ASOP'\n",
    "\n",
    "# Call the function to load and extract text from PDFs in the specified folder\n",
    "docs = load_pdfs_from_folder(folder_path)\n",
    "\n",
    "# Create a text splitter object with specified parameters\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200,\n",
    "    length_function=len,)\n",
    "\n",
    "# Split the documents into chunks using the text splitter\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a Chroma vector database from the document splits, using OpenAIEmbeddings for embedding\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings(), \n",
    "                                    persist_directory=\"../data/chroma_db1\")\n",
    "''' # Uncomment when creating your own vector database for the first time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46105ba",
   "metadata": {},
   "source": [
    "# 3. Retrieve from the Vector DB\n",
    "Once a vector database is created, Section 2 can be commented out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9abd0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Chroma vector database with specified parameters\n",
    "vectorstore = Chroma(embedding_function=OpenAIEmbeddings(), \n",
    "                     persist_directory=\"../data/chroma_db1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9744b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve and RAG chain\n",
    "\n",
    "# Create a retriever using the vector database as the search source\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", \n",
    "                                     search_kwargs={'k': 6, 'lambda_mult': 0.25}) \n",
    "# Use MMR (Maximum Marginal Relevance) to find a set of documents that are both similar to the input query and diverse among themselves\n",
    "# Increase the number of documents to get, and increase diversity (lambda mult 0.5 being default, 0 being the most diverse, 1 being the least)\n",
    "\n",
    "# Load the RAG (Retrieval-Augmented Generation) prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Create a ChatOpenAI language model for augmented generation\n",
    "# llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", \n",
    "#                 temperature=0) # context window size 16k for GPT 3.5 Turbo\n",
    "\n",
    "# Create a local large language model for augmented generation\n",
    "llm = Ollama(model=\"solar:10.7b-instruct-v1-q5_K_M\")\n",
    "\n",
    "# Define a function to format the documents with their sources and pages\n",
    "def format_docs_with_sources(docs):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    sources_pages = \"\\n\".join(f\"{doc.metadata['source']} (Page {doc.metadata['page'] + 1})\" for doc in docs)\n",
    "    # Added 1 to the page number assuming 'page' starts at 0 and we want to present it in a user-friendly way\n",
    "\n",
    "    return f\"Documents:\\n{formatted_docs}\\n\\nSources and Pages:\\n{sources_pages}\"\n",
    "\n",
    "# Create a RAG chain using the formatted documents as the context\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs_with_sources(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a parallel chain for retrieving and generating answers\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9243e34a",
   "metadata": {},
   "source": [
    "# 4. Generate Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ceedb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output():\n",
    "    # Prompt the user for a question on ASOP\n",
    "    usr_input = input(\"What is your question on ASOP?: \")\n",
    "\n",
    "    # Invoke the RAG chain with the user input as the question\n",
    "    output = rag_chain_with_source.invoke(usr_input)\n",
    "\n",
    "    # Generate the Markdown output with the question, answer, and context\n",
    "    markdown_output = \"### Question\\n{}\\n\\n### Answer\\n{}\\n\\n### Context\\n\".format(output['question'], output['answer'])\n",
    "\n",
    "    last_page_content = None  # Variable to store the last page content\n",
    "    i = 1 # Source indicator\n",
    "\n",
    "    # Iterate over the context documents to format and include them in the output\n",
    "    for doc in output['context']:\n",
    "        current_page_content = doc.page_content.replace('\\n', '  \\n')  # Get the current page content\n",
    "        \n",
    "        # Check if the current content is different from the last one\n",
    "        if current_page_content != last_page_content:\n",
    "            markdown_output += \"- **Source {}**: {}, page {}:\\n\\n{}\\n\".format(i, doc.metadata['source'], doc.metadata['page'], current_page_content)\n",
    "            i = i + 1\n",
    "        last_page_content = current_page_content  # Update the last page content\n",
    "    \n",
    "    # Display the Markdown output\n",
    "    display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54daa6",
   "metadata": {},
   "source": [
    "### Example questions related to ASOPs\n",
    "- explain ASOP No. 14\n",
    "- How are expenses relfected in cash flow testing based on ASOP No. 22?\n",
    "- What is catastrophe risk?\n",
    "- When do I update assumptions?\n",
    "- What should I do when I do not have credible data to develop non-economic assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36183436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is your question on ASOP?:  What is catastrophe risk?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Question\n",
       "What is catastrophe risk?\n",
       "\n",
       "### Answer\n",
       "Catastrophe risk refers to the potential for significant losses resulting from relatively infrequent and often extreme events, both natural phenomena like earthquakes or hurricanes, and manmade incidents like explosions causing toxic material release. These events can significantly disrupt historical experience in terms of insurance data and claims, necessitating adjustments when estimating catastrophe provisions in rates. Catastrophe modeling is commonly used to assess such risks and their impact on insurer finances and risk management practices.\n",
       "\n",
       "### Context\n",
       "- **Source 1**: ../data/ASOP/asop039_156.pdf, page 5:\n",
       "\n",
       "b. Infrequent Occurrence—Some events that occur infrequently have the potential to   \n",
       "produce losses that can significantly distor t the historical experience. An example   \n",
       "of such an event is an explosion that resu lts in the release of toxic material. If the   \n",
       "experience data contain such events, using this experience data without   \n",
       "adjustment may overstate the catastrophe pr ovision in the rates. If the experience   \n",
       "data do not contain such events, using this experience data without adjustment may understate the catastrophe provision in the rates.    \n",
       " 3.2 Identification of Catastrophe Losses  \n",
       "—The actuary should identify, where practicable, the   \n",
       "catastrophe losses in the hist orical insurance data. In doing so, the actuary should   \n",
       "consider how accurately the catas trophe losses can be identifi ed, and the extent to which   \n",
       "they may have a material impact on the results of the analysis.\n",
       "- **Source 2**: ../data/ASOP/asop039_156.pdf, page 11:\n",
       "\n",
       "damage ratio, i.e., ratio of losse s to amount of insurance. These damage ratios are applied to the   \n",
       "current or projected amounts of insurance and, wh en adjusted by the estimated frequencies of the   \n",
       "specific catastrophes, produce the expected catastrophe losses.   \n",
       " Since our knowledge of catastrophes is not comp lete and is still evolving, computer simulation   \n",
       "models are also evolving. The expected catastrop he losses calculated from these models can be\n",
       "- **Source 3**: ../data/ASOP/asop039_156.pdf, page 10:\n",
       "\n",
       "Examples of such issues include coverage ch anges, such as the greater use of guaranteed   \n",
       "replacement cost on homeowner po licies or the use of separate wind deductibles; the emergence   \n",
       "of state-run catastrophe f unds; and the availability of catastrophe options.\n",
       "- **Source 4**: ../data/ASOP/asop039_156.pdf, page 15:\n",
       "\n",
       "ASOP No. 39—June 2000    \n",
       "   \n",
       " 12standard has been retitled to specify that it applies to property/casualty insurance   \n",
       "ratemaking. The services referred to for risk fi nancing systems, such as self-insurance and   \n",
       "securitization products, are considered to be  ratemaking when estimates for future costs   \n",
       "are being determined.    \n",
       "    \n",
       "           Section 2.  Definitions  \n",
       "   \n",
       "  Section 2.1, Catastrophe—One commentator believed  that the definition of catastrophe should   \n",
       "relate to how the event or phenomenon violat ed the general insura nce ratemaking model   \n",
       "assumption of independent events.  The subcommittee believes th at the use of a qualitative   \n",
       "definition is more broadly applicable and us eful in terms of current accepted practices.    \n",
       "   \n",
       "Another commentator believed th at the phrase “or natural phe nomenon”should be removed, as   \n",
       "the phrase “relatively infrequent events”  included natural and manmade phenomena. The   \n",
       "subcommittee agreed and deleted the word “natural” from the definition.\n",
       "- **Source 5**: ../data/ASOP/asop038_201.pdf, page 14:\n",
       "\n",
       "models often entering financial statements directly.   \n",
       "   \n",
       "Lastly, due to the evolution of enterprise risk management (ERM) practices and regulations, there   \n",
       "has been increased use of catastrophe modeling as part of insurer stress testing and risk   \n",
       "management across all practice areas. This trend is likely to continue to evolve and heighten in   \n",
       "light of the emergence of the novel coronavirus and the COVID-19 pandemic.\n",
       "- **Source 6**: ../data/ASOP/asop038_201.pdf, page 9:\n",
       "\n",
       "to the results of the actuarial analysis.   \n",
       "   \n",
       "3.2 Catastrophe Models Developed by Experts—When selecting, using, reviewing, or   \n",
       "evaluating a catastrophe model  developed by experts, the actuary should take into   \n",
       "account the following:   \n",
       "   \n",
       " a. whether the individual or individuals who developed the catastrophe model  are   \n",
       "experts in the applicable field;   \n",
       "   \n",
       " b. the extent to which the catastrophe model  has been reviewed or validated by   \n",
       "experts in the applicable field, including known differences of opinion among\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5938771",
   "metadata": {},
   "source": [
    "# 5. References\n",
    "- https://www.actuarialstandardsboard.org/standards-of-practice/\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/quickstart\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/sources\n",
    "- https://chat.langchain.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd6605-deab-4be4-afaf-563098577bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
