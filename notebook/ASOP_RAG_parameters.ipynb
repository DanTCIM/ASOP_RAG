{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57226901",
   "metadata": {},
   "source": [
    "# Actuarial Standards of Practice (ASOP) Q&A Machine using Retrieval Augmented Generation (RAG)\n",
    "This project aims to create a Retrieval-Augmented Generation (RAG) process for actuaries to ask questions on a set of Actuarial Standards of Practice (ASOP) documents. The RAG process utilizes the power of the Large Language Model (LLM) to provide answers to questions on ASOPs.\n",
    "\n",
    "However, RAG is not without challenges, i.e., hallucination and inaccuracy. This code allows verifiability by providing the context it used to arrive at those answers. This process enables actuaries to validate the information provided by the LLM, empowering them to make informed decisions. By combining the capabilities of LLM with verifiability, this code offers actuaries a robust tool to leverage LLM technology effectively and extract maximum value.\n",
    "\n",
    "The current example uses OpenAI's GPT 3.5 turbo AND outputs results using different parameters for comparison purposes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c8d88",
   "metadata": {},
   "source": [
    "# 1. Initial Setup\n",
    "This setup includes loading environment variables from a `.env` file, setting the required environment variables, and importing the necessary modules for further processing. It ensures that the code has access to the required APIs and functions for the subsequent tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fc759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial set up\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the variables from .env file and set the API key (or user may manually set the API key)\n",
    "load_dotenv()  # This loads the variables from .env (not part of repo)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" # use when you want to debug or monitor the performance of your langchain applications\n",
    "#os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGCHAIN_API_KEY') # use when accessing cloud-based language models or services that langchain integrates with\n",
    "\n",
    "# Import the necessary modules\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel # for RAG with source\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import glob\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35630ca8-a707-4445-b8a2-661fe3312d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_OpenAI\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "db_directory = \"../data/chroma_db1\"\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", \n",
    "                 temperature=0) # context window size 16k for GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04673e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Load PDF Files and Convert to a Vector DB\n",
    "1. Create a function to load and extract text from PDF files in a specified folder. It defines a function called `load_pdfs_from_folder()` that takes a folder path as input and returns a list of extracted text documents from the PDF files in that folder.\n",
    "\n",
    "2. In the example, the folder path `../data/ASOP` is used, but you can modify it to point to your desired folder.\n",
    "\n",
    "3. By calling the `load_pdfs_from_folder()` function with the folder path, the code loads the PDF files, extracts the text using the PyPDFLoader, and stores the extracted text documents in the `docs` list.\n",
    "\n",
    "4. After loading and extracting the text, a `RecursiveCharacterTextSplitter` object is created with specific parameters for chunking the documents. The `split_documents()` method is then used to split the documents into smaller chunks based on the specified parameters.\n",
    "\n",
    "5. Finally, a Chroma vectorstore is created from the document splits. The vectorstore uses the defined embedding model for embedding the chunks and is saved to the predefined directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3b91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only when the DB directory is empty\n",
    "if not os.path.exists(db_directory) or not os.listdir(db_directory):\n",
    "    # Define a function to load and extract text from PDFs in a folder\n",
    "    def load_pdfs_from_folder(folder_path):\n",
    "        # Get a list of PDF files in the specified folder\n",
    "        pdf_files = glob.glob(f\"{folder_path}/*.pdf\")\n",
    "        docs = []\n",
    "        for pdf_file in pdf_files:\n",
    "            # Load the PDF file using the PyPDFLoader\n",
    "            loader = PyPDFLoader(pdf_file) \n",
    "            # Extract the text from the PDF and add it to the docs list\n",
    "            docs.extend(loader.load())\n",
    "        return docs\n",
    "    \n",
    "    # Example folder path\n",
    "    folder_path = '../data/ASOP'\n",
    "    \n",
    "    # Call the function to load and extract text from PDFs in the specified folder\n",
    "    docs = load_pdfs_from_folder(folder_path)\n",
    "    \n",
    "    # Create a text splitter object with specified parameters\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=200,\n",
    "        length_function=len,)\n",
    "    \n",
    "    # Split the documents into chunks using the text splitter\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # Create a Chroma vector database from the document splits, using OpenAIEmbeddings for embedding\n",
    "    vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                        embedding=embeddings_model, \n",
    "                                        persist_directory=db_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46105ba",
   "metadata": {},
   "source": [
    "# 3. Retrieve from the Vector DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9abd0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Chroma vector database with specified parameters\n",
    "vectorstore = Chroma(embedding_function=embeddings_model, \n",
    "                     persist_directory=db_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed352868-2e7b-4afd-912f-a5f6b568d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where you may change the parameters\n",
    "n_k = 4 # Number of documents to output (different from the number of documents to fetch in the algorithm)\n",
    "lambda_1 = 1.0 # 1 being the least diverse, 0 being the most diverse\n",
    "lambda_2 = 0.0 # 1 being the least diverse, 0 being the most diverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9744b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve and RAG chain\n",
    "\n",
    "# Create a retriever using the vector database as the search source\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", \n",
    "                                     search_kwargs={'k': n_k, 'lambda_mult': lambda_1}) \n",
    "\n",
    "## Alternative method using local open-source LLM\n",
    "retriever_ALT = vectorstore.as_retriever(search_type=\"mmr\", \n",
    "                                     search_kwargs={'k': n_k, 'lambda_mult': lambda_2})\n",
    "\n",
    "# Use MMR (Maximum Marginal Relevance) to find a set of documents that are both similar to the input query and diverse among themselves\n",
    "# Increase the number of documents to get, and increase diversity (lambda mult 0.5 being default, 0 being the most diverse, 1 being the least)\n",
    "\n",
    "# Load the RAG (Retrieval-Augmented Generation) prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define a function to format the documents with their sources and pages\n",
    "def format_docs_with_sources(docs):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    sources_pages = \"\\n\".join(f\"{doc.metadata['source']} (Page {doc.metadata['page'] + 1})\" for doc in docs)\n",
    "    # Added 1 to the page number assuming 'page' starts at 0 and we want to present it in a user-friendly way\n",
    "\n",
    "    return f\"Documents:\\n{formatted_docs}\\n\\nSources and Pages:\\n{sources_pages}\"\n",
    "\n",
    "# Create a RAG chain using the formatted documents as the context\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs_with_sources(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a parallel chain for retrieving and generating answers\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)\n",
    "\n",
    "## Alternative method using different parameters\n",
    "# Create a parallel chain for retrieving and generating answers\n",
    "rag_chain_with_source_ALT = RunnableParallel(\n",
    "    {\"context\": retriever_ALT, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9243e34a",
   "metadata": {},
   "source": [
    "# 4. Generate Q&A Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ceedb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output():\n",
    "    # Prompt the user for a question on ASOP\n",
    "    usr_input = input(\"What is your question on ASOP?: \")\n",
    "\n",
    "    # Invoke the RAG chain with the user input as the question\n",
    "    output = rag_chain_with_source.invoke(usr_input)\n",
    "    output_ALT = rag_chain_with_source_ALT.invoke(usr_input)\n",
    "\n",
    "    # Generate the Markdown output with the question, answer, and context\n",
    "    markdown_output = \"### Question\\n{}\\n\\n### First Answer with {}\\n{}\\n\\n\".format(output['question'], lambda_1, output['answer'])\n",
    "    markdown_output += \"### Second Answer with {}\\n{}\\n\\n### First Context\\n\".format(lambda_2, output_ALT['answer'])\n",
    "\n",
    "    last_page_content = None  # Variable to store the last page content\n",
    "    i = 1 # Source indicator\n",
    "\n",
    "    # Iterate over the context documents to format and include them in the output\n",
    "    for doc in output['context']:\n",
    "        current_page_content = doc.page_content.replace('\\n', '  \\n')  # Get the current page content\n",
    "        \n",
    "        # Check if the current content is different from the last one\n",
    "        if current_page_content != last_page_content:\n",
    "            markdown_output += \"- **First Source {}**: {}, page {}:\\n\\n{}\\n\".format(i, doc.metadata['source'], doc.metadata['page'], current_page_content)\n",
    "            i = i + 1\n",
    "        last_page_content = current_page_content  # Update the last page content\n",
    "\n",
    "    markdown_output += \"\\n\\n### Second Context\\n\"\n",
    "\n",
    "    last_page_content = None  # Variable to store the last page content\n",
    "    i = 1 # Source indicator\n",
    "\n",
    "    # Iterate over the context documents to format and include them in the output\n",
    "    for doc in output_ALT['context']:\n",
    "        current_page_content = doc.page_content.replace('\\n', '  \\n')  # Get the current page content\n",
    "        \n",
    "        # Check if the current content is different from the last one\n",
    "        if current_page_content != last_page_content:\n",
    "            markdown_output += \"- **Second Source {}**: {}, page {}:\\n\\n{}\\n\".format(i, doc.metadata['source'], doc.metadata['page'], current_page_content)\n",
    "            i = i + 1\n",
    "        last_page_content = current_page_content  # Update the last page content\n",
    "\n",
    "    \n",
    "    # Display the Markdown output\n",
    "    display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54daa6",
   "metadata": {},
   "source": [
    "# Example questions related to ASOPs\n",
    "- explain ASOP No. 14\n",
    "- What are the considerations in choosing what methods to use for asset adequacy testing?\n",
    "- How are expenses reflected in cash flow testing based on ASOP No. 22?\n",
    "- What is catastrophe risk?\n",
    "- When do I update assumptions?\n",
    "- What should I do when I do not have credible data to develop non-economic assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36183436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is your question on ASOP?:  What should I do when I do not have credible data to develop non-economic assumptions?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Question\n",
       "What should I do when I do not have credible data to develop non-economic assumptions?\n",
       "\n",
       "### First Answer with 1.0\n",
       "When credible data is not available to develop non-economic assumptions, consider other information sources such as pricing or reserving practices of similar insurance businesses. Obtain input from individuals with relevant expertise and ensure internal consistency of assumptions used in the appraisal. Use professional judgment and available sources of data to set assumptions when no relevant historical experience is available.\n",
       "\n",
       "### Second Answer with 0.0\n",
       "When credible data is not available to develop non-economic assumptions, consider other information sources such as pricing practices in the insurance business or the experience of other similar insurance companies. Obtain input from individuals with relevant expertise and ensure that each set of assumptions used is internally consistent. If additional expertise is needed, seek input from knowledgeable individuals and give due weight to their input.\n",
       "\n",
       "### First Context\n",
       "- **First Source 1**: ../data/ASOP/asop019_137.pdf, page 7:\n",
       "\n",
       "information is available. When experience of the business is unavailable or insufficient to   \n",
       "provide a credible basis on which to develo p assumptions, the actuary should consider   \n",
       "other information sources in setting assumptions. Other information sources may include the pricing or reserving practices applicable  to the insurance business and the available   \n",
       "experience of other insurance businesses with comparable policies or contracts, markets,   \n",
       "and operating environment.    \n",
       "  In developing assumptions for which the actuary believes additional expertise is needed,   \n",
       "the actuary should obtain necessary input from persons possessing the relevant   \n",
       "knowledge or expertise, and should gi ve due weight to their input.    \n",
       "  When setting assumptions for use in an appraisal, the actuary should take reasonable   \n",
       "steps to ensure that each set of assump tions used is internally consistent.   \n",
       "   \n",
       "3.4 Discount Rate  \n",
       "⎯If the appraisal is based on the di scounted value of projected earnings,\n",
       "- **First Source 2**: ../data/ASOP/asop027_197.pdf, page 10:\n",
       "\n",
       "d. take into account other general considerations, when applicable (section 3.5); and   \n",
       "   \n",
       " e. select a reasonable assumption (section 3.6).    \n",
       "   \n",
       "After completing these steps for each economic assumption, the actuary should review the   \n",
       "set of economic assumptions for consistency (section 3.12) and make appropriate   \n",
       "adjustments if necessary.   \n",
       "   \n",
       "3.4 Relevant Data—To evaluate relevant data, the actuary should review appropriate recent   \n",
       "and long-term historical economic data. The actuary should not give undue weight to recent   \n",
       "experience. The actuary should take into account the possibility that some historical   \n",
       "economic data may not be appropriate for use in developing assumptions for future periods   \n",
       "due to changes in the underlying environment.     \n",
       "   \n",
       "3.5 General Considerations—The actuary should take into account the following when   \n",
       "applicable:   \n",
       "   \n",
       " 3.5.1  Adverse Deviation or Plan Provisions That Are Difficult to Measure—Depending\n",
       "- **First Source 3**: ../data/ASOP/asop054_193.pdf, page 11:\n",
       "\n",
       "3.4.1.3 Assumptions When There Is No Relevant Historical Experience  \n",
       "—In   \n",
       "some instances, no relevant histori cal experience is available to the   \n",
       "actuary. In this situation, the actu ary should use prof essional judgment,   \n",
       "considering available sources of data, when setting assumptions.    \n",
       "   \n",
       "3.4.2 Assumption Margins—The actuary shoul d consider the appropriateness of   \n",
       "including a margin in the assumptions. When setting a margin, the actuary should   \n",
       "consider the following:    \n",
       "   \n",
       "a. the degree to which there is uncerta inty around the assumptions due to   \n",
       "lack of relevant, credible company or  industry experience data to support   \n",
       "the assumptions;\n",
       "- **First Source 4**: ../data/ASOP/asop052_189.pdf, page 13:\n",
       "\n",
       "company.  Where no relevant  and credible company experience is available, the actuary should use   \n",
       "professional judgment in advising on the adop tion and modification of  other sources of   \n",
       "experience data. Examples of items that may result in modifications to the experience   \n",
       "data include the company’s underwriti ng and administrative practices, market   \n",
       "demographics, product design, and econom ic and regulatory environments.   \n",
       " Section 9 of VM-20 requires sensitivity testing to determine which assumptions have the   \n",
       "most significant impact on reserves. The ac tuary should consid er performing more   \n",
       "extensive analyses in setting assumptions that have a significant impact on valuation   \n",
       "results.  The actuary should consider granularity in setting assumptions given the model   \n",
       "structure. The actuary should us e professional judgment to set granularity  to reflect   \n",
       "expected experience appropriately.\n",
       "\n",
       "\n",
       "### Second Context\n",
       "- **Second Source 1**: ../data/ASOP/asop019_137.pdf, page 7:\n",
       "\n",
       "information is available. When experience of the business is unavailable or insufficient to   \n",
       "provide a credible basis on which to develo p assumptions, the actuary should consider   \n",
       "other information sources in setting assumptions. Other information sources may include the pricing or reserving practices applicable  to the insurance business and the available   \n",
       "experience of other insurance businesses with comparable policies or contracts, markets,   \n",
       "and operating environment.    \n",
       "  In developing assumptions for which the actuary believes additional expertise is needed,   \n",
       "the actuary should obtain necessary input from persons possessing the relevant   \n",
       "knowledge or expertise, and should gi ve due weight to their input.    \n",
       "  When setting assumptions for use in an appraisal, the actuary should take reasonable   \n",
       "steps to ensure that each set of assump tions used is internally consistent.   \n",
       "   \n",
       "3.4 Discount Rate  \n",
       "⎯If the appraisal is based on the di scounted value of projected earnings,\n",
       "- **Second Source 2**: ../data/ASOP/asop051_188.pdf, page 24:\n",
       "\n",
       "ASOP No. 51—September 2017   \n",
       "   \n",
       "   \n",
       " 18Section 3.4, Assumptions for Assess ment of Risk (now  section 3.5)   \n",
       "Comment   \n",
       "   \n",
       "   \n",
       "Response One commentator suggested that empirical data be used to select assumptions rather than using   \n",
       "professional judgment.   \n",
       "   \n",
       "The reviewers note that the section Assumptions for Assessment of Risk reads “The assumptions used for assessment of risk may be based on economic and demographic data and analyses,” but   \n",
       "believe “the actuary should use professional judgment in selecting [these] assumptions” and made   \n",
       "no change in response to this comment.   \n",
       "Comment    \n",
       "   \n",
       "Response Two commentators suggested the term “plausible” is  not clear and also that implausible outcomes   \n",
       "should be considered.   \n",
       "   \n",
       "The reviewers believe the term “plausible,” combined with the requirement for the actuary to use professional judgment, is appropriate for this standard and made no change in response to this   \n",
       "comment.   \n",
       "Comment\n",
       "- **Second Source 3**: ../data/ASOP/asop007_128.pdf, page 16:\n",
       "\n",
       "3.10.6 Limitations of Models, Assumptions, and Data  \n",
       "—Cash flow estimates can vary   \n",
       "considerably as a result of the model use d, the assumptions selected, and the data.   \n",
       "When results are highly volatile, additional analysis may be appropriate.   \n",
       " 3.11 Negative Interim Earnings  \n",
       "—The actuary should consider the impact of any negative interim   \n",
       "earnings during the cash flow projection period, if it is appropriate for the purpose of the   \n",
       "analysis.   \n",
       "   Section 4.  Communications and Disclosures  \n",
       "   \n",
       " 4.1 Reliance on Others for Data, Projections, and Supporting Analysis  \n",
       "—The actuary may rely   \n",
       "on data, projections, and supporting analysis supplied by others. In doing so, the actuary   \n",
       "should disclose both the fact and the extent of such reliance. Such disclosure may follow the   \n",
       "forms prescribed in the applicable NAIC model laws and regulations. The accuracy and comprehensiveness of data, projections, or supporting analysis supplied by others are the\n",
       "- **Second Source 4**: ../data/ASOP/asop046_165.pdf, page 12:\n",
       "\n",
       "ASOP No. 46—September 2012    \n",
       "   \n",
       " 7b. prices in the marketplace;   \n",
       "   \n",
       "c. opinions of other experts;   \n",
       "   \n",
       "d. the fit of the assumed dist ribution to available data;   \n",
       "   \n",
       "e. the ability of the assumed distribution to reflect possible extreme values;    \n",
       "   \n",
       "f. sensitivity of results to changes in assumptions;    \n",
       "   \n",
       "g.  internal consistency of the assumptions; and  h. consistency in the app lication of assumptions.   \n",
       " 3.3.5   Validation of the Economic Capital Model  \n",
       "—Economic capital is often   \n",
       "determined based on the resu lts of stochastic models that produce a large number   \n",
       "of outcomes. The actuary should devise a ppropriate tests of the distribution of   \n",
       "outcomes calculated by the model (for example, in comparison to the range of results in similar models or to histori cal outcomes over time) and the sensitivity of   \n",
       "those distributions to changes in the assumptions and parameters. The actuary should also perform validation tests to determine whether the model results are\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5938771",
   "metadata": {},
   "source": [
    "# 5. References\n",
    "- https://www.actuarialstandardsboard.org/standards-of-practice/\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/quickstart\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/sources\n",
    "- https://python.langchain.com/docs/integrations/text_embedding/\n",
    "- https://docs.gpt4all.io/gpt4all_python_embedding.html#gpt4all.gpt4all.Embed4All\n",
    "- https://chat.langchain.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-RAG",
   "language": "python",
   "name": "python-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
